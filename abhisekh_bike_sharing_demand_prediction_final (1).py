# -*- coding: utf-8 -*-
"""abhisekh_Bike sharing demand prediction final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ridG6B7Gig4AhSqlH5-1cZ26W9grqA8X

# **Project Name**    - Seoul Bike sharing regression problem

##### **Project Type**    - Regression
##### **Contribution**    - Team-Data Titans

# **Project Summary -**

Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes. With the dataset containing different features like temperature, radiation, wind,snowfall etc and the corresponding bikes rented at particular time. So first we performed EDA to find the hidden insights from the data. We observed that in summer people tend to book more compared to other seasons and theres particular time of a day when the demand for bike is high. after EDA we applied ML algorithms on a 80:20 training and testing data set and we saw that Xgboost with hyperparameter tunning gives the best result followed by Catboost

# **GitHub Link -**

Provide your GitHub Link here.

# **Problem Statement**

**Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes.**

# **General Guidelines** : -

1.   Well-structured, formatted, and commented code is required. 
2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. 
     
     The additional credits will have advantages over other students during Star Student selection.
       
             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go
                       without a single error logged. ]

3.   Each and every logic should have proper comments.
4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.
        

```
# Chart visualization code
```
            

*   Why did you pick the specific chart?
*   What is/are the insight(s) found from the chart?
* Will the gained insights help creating a positive business impact? 
Are there any insights that lead to negative growth? Justify with specific reason.

5. You have to create at least 15 logical & meaningful charts having important insights.


[ Hints : - Do the Vizualization in  a structured way while following "UBM" Rule. 

U - Univariate Analysis,

B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)

M - Multivariate Analysis
 ]





6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.


*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.


*   Cross- Validation & Hyperparameter Tuning

*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.

# ***Let's Begin !***

## ***1. Know Your Data***

### Import Libraries
"""

# Importing Libraries

# Commented out IPython magic to ensure Python compatibility.
import numpy as np 
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import datetime as dt
from google.colab import drive
import seaborn

"""### Dataset Loading"""

# Loading Dataset
drive.mount('/content/drive')
path='/content/drive/MyDrive/AlmaBetter/machine learning/capstone project bike sharing/SeoulBikeData.csv'
pd.pandas.set_option('display.max_columns',None)
data=pd.read_csv(path,encoding='latin')

"""### Dataset First View"""

# Dataset First Look
data.head()

"""### Dataset Rows & Columns count"""

# Dataset Rows & Columns count
data.shape

"""### Dataset Information"""

# Dataset Info
data.info()

"""#### Duplicate Values"""

# Dataset Duplicate Value Count
value=len(data[data.duplicated()])
print(f"The number of duplicate values in the data set is = {value}")

"""#### Missing Values/Null Values"""

# Missing Values/Null Values Count
Null_values=data.isnull().sum()
print(f'The number of missing value in each feature is- \n{Null_values}')

missing = pd.DataFrame((data.isnull().sum())*100/data.shape[0]).reset_index()
plt.figure(figsize=(16,5))
ax = sns.pointplot('index',0,data=missing)
plt.xticks(rotation =90,fontsize =7)
plt.title("Percentage of Missing values")
plt.ylabel("Percentage")
plt.show()

"""### What did you know about your dataset?

The above dataset has a shape of (8760,14) which means it has 8760 rows of entries and 14 columns which represents the features in our dataset.
Also the dataset has 10 numerical and 4 categorial features in the dataset.
The dataset doesnot have any null value or missing values.
Also it does not have any duplicate values.

## ***2. Understanding Your Variables***
"""

# Dataset Columns
columns=list(data.columns)

# Dataset Describe
data.describe().T

"""### Variables Description 
* ### Date : year-month-day
* ### Rented Bike count - Count of bikes rented at each hour
* ### Hour - Hour of he day
* ### Temperature-Temperature in Celsius
* ### Humidity - %
* ### Windspeed - m/s
* ### Visibility - 10m
* ### Dew point temperature - Celsius
* ### Solar radiation - MJ/m2
* ### Rainfall - mm
* ### Snowfall - cm
* ### Seasons - Winter, Spring, Summer, Autumn
* ### Holiday - Holiday/No holiday
* ### Functional Day - NoFunc(Non Functional Hours), Fun(Functional hours)

### Check Unique Values for each variable.
"""

# Check Unique Values for each variable.
data.nunique()

"""## 3. ***Data Wrangling***

### Data Wrangling Code
"""

#Rename the complex columns name
bike_data=data.rename(columns={'Rented Bike Count':'Rented_bike_count',
                                'Temperature(°C)':'Temperature',
                                'Humidity(%)':'Humidity',
                                'Wind speed (m/s)':'Wind_speed',
                                'Visibility (10m)':'Visibility',
                                'Dew point temperature(°C)':'Dew_point_temperature',
                                'Solar Radiation (MJ/m2)':'Solar_radiation',
                                'Rainfall(mm)':'Rainfall',
                                'Snowfall (cm)':'Snowfall',
                                'Functioning Day':'Functioning_Day'})

# Changing the "Date" column into three parts i.e-"year","month","day" columns respectively
bike_data['Date'] = bike_data['Date'].apply(lambda x: 
                                    dt.datetime.strptime(x,"%d/%m/%Y"))

bike_data['year'] = bike_data['Date'].dt.year
bike_data['month'] = bike_data['Date'].dt.month
bike_data['day'] = bike_data['Date'].dt.day_name()

#creating a new column of "weekdays_weekend" and drop the column "Date","day","year"
bike_data['weekdays_weekend']=bike_data['day'].apply(lambda x : 1 if x=='Saturday' or x=='Sunday' else 0 )
bike_data=bike_data.drop(columns=['Date','day','year'],axis=1)

bike_data.info()

#Change the int64 column into catagory column
cols=['Hour','month','weekdays_weekend']
for col in cols:
  bike_data[col]=bike_data[col].astype('category')

"""### What all manipulations have you done and insights you found?

Here in the data wrangling I have changed the names of the columns so that it will be easier to run and write the codes. Also extracted year,month and day data from the date feature to make the dataset more useful for analysis. I have also droped the year date day features because of their non contributing nature towards our problem statement.
Some Insights which i gained is that-
1)we now have 12 numerical features and 3 categorical feautres in total but in that the hour, month, and weekdays_weekend category all classified as numerical feauteres and hence we have to change it to categorical so that we can make right use of these features for data analysis.

## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***
"""

bike_data.columns

"""Why do we perform EDA?

An EDA is a complete examination meant to uncover the underlying structure of a data set and is important because it exposes trends, patterns, and relationships which are not usually be known just looking at the dataset.

**Univarate analysis**

#### Chart - 1

***Bike Sharing count over the months***
"""

# Chart - 1 visualization code
#anlysis of data by vizualisation
fig,ax=plt.subplots(figsize=(20,8))
sns.barplot(data=bike_data,x='month',y='Rented_bike_count',ax=ax,capsize=.2)
ax.set(title='Count of Rented bikes acording to Month ')

"""##### 1. Why did you pick the specific chart?

We have used this type of chart because of the interval scale(month) we have in the result and histogram is best suited to represent that graphically

##### 2. What is/are the insight(s) found from the chart?

Major insight found from the graph is that in the month may we have seen most number of bikes rented whereas january seen the least.

##### 3. Will the gained insights help creating a positive business impact? 
Are there any insights that lead to negative growth? Justify with specific reason.

Yes the gained insight can help to create a positive impact as that would boost and motivate the company to plan its strategy according to this customers previous trend

No there is no insight that lead to negative growth

#### Chart - 2
"""

# Chart - 2 visualization code
#anlysis of data by vizualisation
fig,ax=plt.subplots(figsize=(10,8))
sns.barplot(data=bike_data,x='weekdays_weekend',y='Rented_bike_count',ax=ax,capsize=.2)
ax.set(title='Count of Rented bikes acording to weekdays and weekend ')

"""##### 1. Why did you pick the specific chart?

Here our result works on either 2 parameters and this type of graph is one of best and simple way to represent it

##### 2. What is/are the insight(s) found from the chart?

Top insight gained is that people rented bike more on non weekend days than weekend days

##### 3. Will the gained insights help creating a positive business impact? 
Are there any insights that lead to negative growth? Justify with specific reason.

It can create a positive impact as it depicts the nature of customer and bussinesses can model their bussiness according to these nature to create profit

No insight here leads to negative growth

#### Chart - 3
"""

# Chart - 3 visualization code
#anlysis of data by vizualisation
fig,ax=plt.subplots(figsize=(20,8))
sns.pointplot(data=bike_data,x='Hour',y='Rented_bike_count',hue='weekdays_weekend',ax=ax)
ax.set(title='Count of Rented bikes acording to weekdays_weekend ')

"""##### 1. Why did you pick the specific chart?

Here we selected this type of chart because we have 2 parameters to compare with each other over a period of time so by this way we can easily capture the difference between the two

##### 2. What is/are the insight(s) found from the chart?

major insight is that on non weekend days we can a specific boost in demand for bikes at a particular hour like 8 am and also at 6-7 pm

##### 3. Will the gained insights help creating a positive business impact? 
Are there any insights that lead to negative growth? Justify with specific reason.

This insight will help in formulating strategy and would help to take better decision with this information

#### Chart - 4
"""

# Chart - 4 visualization code
#anlysis of data by vizualisation
fig,ax=plt.subplots(figsize=(20,8))
sns.barplot(data=bike_data,x='Hour',y='Rented_bike_count',ax=ax,capsize=.2)
ax.set(title='Count of Rented bikes acording to Hour ')

"""##### 1. Why did you pick the specific chart?

Here we selected this type of chart because we have a parameter value to compare over a period of time so by this way we can easily understand its pattern

##### 2. What is/are the insight(s) found from the chart?

Some insights are at 8 th hour and 18th hour of day we see huge demand in bike sharing

##### 3. Will the gained insights help creating a positive business impact? 
Are there any insights that lead to negative growth? Justify with specific reason.

Yes

#### Chart - 5
"""

# Chart - 6 visualization code
#anlysis of data by vizualisation
fig,ax=plt.subplots(figsize=(15,8))
sns.barplot(data=bike_data,x='Seasons',y='Rented_bike_count',ax=ax,capsize=.2)
ax.set(title='Count of Rented bikes acording to Seasons ')

"""##### 1. Why did you pick the specific chart?

Here we selected this type of chart because we have to compare value of rented bike with each season.so by this way we can easily capture the insight quickly

##### 2. What is/are the insight(s) found from the chart?

Summer season is the time when we see highest bikes booked and in winter season its very low

##### 3. Will the gained insights help creating a positive business impact? 
Are there any insights that lead to negative growth? Justify with specific reason.

Yes, it will help create a positive business impact because it shows a very important attribute of its customer cluster

#### Chart - 6
"""

# Chart - 8 visualization code
#anlysis of data by vizualisation
fig,ax=plt.subplots(figsize=(15,8))
sns.barplot(data=bike_data,x='Holiday',y='Rented_bike_count',ax=ax,capsize=.2)
ax.set(title='Count of Rented bikes acording to Holiday ')

"""##### 1. Why did you pick the specific chart?

Here we selected this type of chart because we have to see on which day the rented bike was more and this way fits good to do this visualization.

##### 2. What is/are the insight(s) found from the chart?

People rent bike more on non holidays than they do on holidays

##### 3. Will the gained insights help creating a positive business impact? 
Are there any insights that lead to negative growth? Justify with specific reason.

yes, it will help create a positive impact as this insight is about the user and would help to make strategies based on the insight

#### Chart - 7
"""

# Chart - 9 visualization code
#printing the regression plot for all the numerical features
#assign the numerical coulmn to variable
numerical_columns=list(bike_data.select_dtypes(['int64','float64']).columns)
numerical_features=pd.Index(numerical_columns)
numerical_features

for col in numerical_features:
  fig,ax=plt.subplots(figsize=(10,6))
  sns.regplot(x=bike_data[col],y=bike_data['Rented_bike_count'],scatter_kws={"color": 'orange'}, line_kws={"color": "black"})

"""##### 1. Why did you pick the specific chart?

This chart is generally used for regression plot

##### 2. What is/are the insight(s) found from the chart?

all the features on which we applied reggression plot to create a regression line between the data with respect to target variable and helps to visualize their linear relationship. 

From the above regression plot of all numerical features we see that the columns 'Temperature', 'Wind_speed','Visibility', 'Dew_point_temperature', 'Solar_Radiation' are positively relation to the target variable.
which means the rented bike count increases with increase of these features.
'Rainfall','Snowfall','Humidity' these features are negatively related with the target variaable which means the rented bike count decreases when these features increase.

##### 3. Will the gained insights help creating a positive business impact? 
Are there any insights that lead to negative growth? Justify with specific reason.

No, this insight is not enough to make any particular change

#### Chart - 8 - Correlation Heatmap
"""

# Correlation Heatmap visualization code
## plot the Correlation matrix
plt.figure(figsize=(20,8))
correlation=bike_data.corr()
mask = np.triu(np.ones_like(correlation, dtype=bool))
sns.heatmap((correlation),mask=mask, annot=True,cmap='coolwarm')

"""##### 1. Why did you pick the specific chart?

Its one of the best way to represent the above result

##### 2. What is/are the insight(s) found from the chart?

We can observe on the heatmap that on the target variable line the most positively correlated variables to the rent are :the temperature,the dew point ,temperature,the solar radiation
And most negatively correlated variables are: Humidity,Rainfall
We see that there is a positive correlation between columns 'Temperature' and 'Dew point temperature' i.e 0.91 , so even if we drop this column, then it will not  affect the outcome of our analysis and they have the same variations. so we can drop the column 'Dew point temperature(°C)'.

#### Chart - 9 - Pair Plot
"""

# Pair Plot visualization code
seaborn.pairplot(bike_data)
# to show
plt.show()

"""##### 1. Why did you pick the specific chart?

pair plot is best used to find relation between two feautures with each other so to find that we have used it

##### 2. What is/are the insight(s) found from the chart?

Its giving same insight about the correlation as in the above analysis of correlation heat map

## ***5. Feature Engineering & Data Pre-processing***

###**Normalisation**

* ***The data normalization (also referred to as data pre-processing) is a basic element of data mining. It means transforming the data, namely converting the source data in to another format that allows processing data effectively. The main purpose of data normalization is to minimize or even exclude duplicated data***
"""

#Distribution plot of Rented Bike Count
plt.figure(figsize=(10,6))
plt.xlabel('Rented_bike_count')
plt.ylabel('Density')
ax=sns.distplot(bike_data['Rented_bike_count'],hist=True ,color="y")
ax.axvline(bike_data['Rented_bike_count'].mean(), color='magenta', linestyle='dashed', linewidth=2)
ax.axvline(bike_data['Rented_bike_count'].median(), color='black', linestyle='dashed', linewidth=2)
plt.show()

"""* ***The above graph shows that Rented Bike Count has moderate right skewness. Since the assumption of linear regression is that 'the distribution of dependent variable has to be normal', so we should perform some operation to make it normal.***

### 2. Handling Outliers
"""

#Boxplot of Rented Bike Count to check outliers
plt.figure(figsize=(10,6))
plt.ylabel('Rented_bike_count')
sns.boxplot(x=bike_data['Rented_bike_count'])
plt.show()

"""* ***The above boxplot shows that we have detect outliers in Rented Bike Count column***"""

#Applying square root to Rented Bike Count to improve skewness
plt.figure(figsize=(10,8))
plt.xlabel('Rented Bike Count')
plt.ylabel('Density')

ax=sns.distplot(np.sqrt(bike_data['Rented_bike_count']), color="y")
ax.axvline(np.sqrt(bike_data['Rented_bike_count']).mean(), color='magenta', linestyle='dashed', linewidth=2)
ax.axvline(np.sqrt(bike_data['Rented_bike_count']).median(), color='black', linestyle='dashed', linewidth=2)

plt.show()

"""* ***Since we have generic rule of applying Square root for the skewed variable in order to make it normal .After applying Square root to the skewed Rented Bike Count, here we get almost normal distribution.***"""

#After applying sqrt on Rented Bike Count check wheater we still have outliers 
plt.figure(figsize=(10,6))

plt.ylabel('Rented_Bike_Count')
sns.boxplot(x=np.sqrt(bike_data['Rented_bike_count']))
plt.show()

bike_data.corr()

"""* ***After applying Square root to the Rented Bike Count column, we find that there is no outliers present.***

##### What all outlier treatment techniques have you used and why did you use those techniques?

Used Square root transformation to make the distribution normal and then used box plot to remove the outliers

### 3. Categorical Encoding

**A dataset may contain various type of values, sometimes it consists of categorical values. So, in-order to use those categorical value for programming efficiently we create dummy variables.**
"""

#Assign all catagoriacla features to a variable
categorical_features=list(bike_data.select_dtypes(['object','category']).columns)
categorical_features=pd.Index(categorical_features)
categorical_features

#creat a copy
bike_data_copy = bike_data

def one_hot_encoding(data, column):
  data = pd.concat([data, pd.get_dummies(data[column], prefix=column, drop_first=True)], axis=1)
  data = data.drop([column], axis=1)
  return data

for col in categorical_features:
    bike_data_copy = one_hot_encoding(bike_data_copy, col)
bike_data_copy.head()

"""### 4. Data Splitting"""

# Split your data to train and test. Choose Splitting ratio wisely.
#Assign the value in X and Y
X = bike_data_copy.drop(columns=['Rented_bike_count'], axis=1)
y = np.sqrt(bike_data_copy['Rented_bike_count'])

#Creat test and train data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20, random_state=0)
print(X_train.shape)
print(X_test.shape)

"""##### What data splitting ratio have you used and why?

Before, fitting any model it is a rule of thumb to split the dataset into a training and test set. This means some proportions of the data will go into training the model and some portion will be used to evaluate how our model is performing on any unseen data. The proportions may vary from 60:40, 70:30, 75:25 depending on the person but mostly used is 80:20 for training and testing respectively. In this step we will split our data into training and testing set using scikit learn library.

## ***6. ML Model Implementation***

### <b> Linear regression
"""

#import the packages
from sklearn.linear_model import LinearRegression
linreg= LinearRegression()
linreg.fit(X_train, y_train)

Modell='LinearRegression'

pred=linreg.predict(X_test)

pred

#check the score
linreg.score(X_train, y_train)

#get the X_train and X-test value
y_pred_train=linreg.predict(X_train)
y_pred_test=linreg.predict(X_test)

y_pred_train

#import the packages
 from sklearn.metrics import mean_squared_error
 from sklearn.metrics import mean_absolute_error
 from sklearn.metrics import r2_score

def Error_estimate(y_train,y_pred_train):
  global MSE,RMSE,MAE,r2,Adjusted_R2
 #calculate MSE
  MSE= mean_squared_error(y_train, y_pred_train)
 #calculate RMSE
  RMSE=np.sqrt(MSE)
 #calculate MAE
  MAE= mean_absolute_error(y_train, y_pred_train)
 #calculate r2 and adjusted r2
  r2= r2_score(y_train, y_pred_train)
  Adjusted_R2 = (1-(1-r2_score(y_train, y_pred_train))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )

def my_outputdic(model_name,MSE,RMSE,MAE,r2,Adjusted_R2):
    # perform some calculations

    # create a dictionary to store the output
    output = {}
    output['model'] = model_name
    output['MSE'] = MSE
    output['RMSE'] = RMSE
    output['MAE'] = MAE
    output['R2'] = r2
    output['Adjusted_R2'] =Adjusted_R2


    return output

Error_estimate(y_train,y_pred_train)

ztrain=my_outputdic(Modell,MSE,RMSE,MAE,r2,Adjusted_R2)

ztrain

Error_estimate((y_pred_test),(y_test))

ztest=my_outputdic(Modell,MSE,RMSE,MAE,r2,Adjusted_R2)

ztest

test_df=pd.DataFrame(ztest,index=[1])

training_df=pd.DataFrame(ztrain,index=[1])

training_df

test_df

### Heteroscadacity
plt.scatter((y_pred_test),(y_test)-(y_pred_test))

#Plot the figure
plt.figure(figsize=(15,10))
plt.plot(y_pred_test)
plt.plot(np.array(y_test))
plt.legend(["Predicted","Actual"])
plt.xlabel('No of Test Data')
plt.show()

# ML Model - 1 Implementation

# Fit the Algorithm

# Predict on the model

"""#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."""

# Visualizing evaluation Metric Score chart

"""### **ELASTIC NET REGRESSION**"""

#import the packages
from sklearn.linear_model import ElasticNet
#a * L1 + b * L2
#alpha = a + b and l1_ratio = a / (a + b)
elasticnet = ElasticNet(alpha=0.1, l1_ratio=0.5)

Modell="Elastic Net Regression"

#FIT THE MODEL
elasticnet.fit(X_train,y_train)

#check the score
elasticnet.score(X_train, y_train)

#get the X_train and X-test value
y_pred_train_en=elasticnet.predict(X_train)
y_pred_test_en=elasticnet.predict(X_test)

y_pred_train_en

Error_estimate(y_train,y_pred_train_en)

ztrain1=my_outputdic(Modell,MSE,RMSE,MAE,r2,Adjusted_R2)

ztrain1

training_df=training_df.append(ztrain1,ignore_index=True)

Error_estimate(y_test,y_pred_test_en)

ztest1=my_outputdic(Modell,MSE,RMSE,MAE,r2,Adjusted_R2)
ztest1

test_df=test_df.append(ztest1,ignore_index=True)

training_df

"""**Looks like our r2 score value is 0.62 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"""

test_df

"""**The r2_score for the test set is 0.619. This means our linear model is  performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**



"""

#Plot the figure
plt.figure(figsize=(15,10))
plt.plot(np.array(y_pred_test_en))
plt.plot((np.array(y_test)))
plt.legend(["Predicted","Actual"])
plt.show()

### Heteroscadacity
plt.scatter((y_pred_test_en),(y_test)-(y_pred_test_en))

"""### **DECISION TREE**

---


"""

#import the packages
from sklearn.tree import DecisionTreeRegressor
decision_regressor = DecisionTreeRegressor(criterion='friedman_mse', max_depth=8,
                      max_features=9, max_leaf_nodes=100,)
decision_regressor.fit(X_train, y_train)

Modell="Decision Tree"

#get the X_train and X-test value
y_pred_train_d = decision_regressor.predict(X_train)
y_pred_test_d = decision_regressor.predict(X_test)

Error_estimate(y_train,y_pred_train_d)
ztrain2=my_outputdic(Modell,MSE,RMSE,MAE,r2,Adjusted_R2)
ztrain2

training_df=training_df.append(ztrain2,ignore_index=True)
training_df

"""Looks like our r2 score value is 0.71 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"""

Error_estimate(y_test,y_pred_test_d)
ztest2=my_outputdic(Modell,MSE,RMSE,MAE,r2,Adjusted_R2)
ztest2

test_df=test_df.append(ztest2,ignore_index=True)
test_df

"""**The r2_score for the test set is 0.66. This means our linear model is  performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**



"""

#Plot the figure
plt.figure(figsize=(15,10))
plt.plot((np.array(y_pred_test_d)))
plt.plot(np.array((y_test)))
plt.legend(["Predicted","Actual"])
plt.show()

### Heteroscadacity
plt.scatter((y_pred_test_d),(y_test)-(y_pred_test_d))

"""### 1. Which Evaluation metrics did you consider for a positive business impact and why?

Answer Here.

### 2. Which ML model did you choose from the above created models as your final prediction model and why?

Answer Here.

### **CATBOOST**
"""

!pip install catboost
from catboost import CatBoostRegressor, Pool, cv
import catboost as ctb
from scipy import stats

model_CBR = ctb.CatBoostRegressor()
model_CBR.fit(X_train, y_train)
print(model_CBR)

Modell="CatBoost"

expected_y = y_test
predicted_y = model_CBR.predict(X_test)

expected_yt = y_train
predicted_yt = model_CBR.predict(X_train)

from sklearn import metrics
print(metrics.r2_score(expected_y, predicted_y))
plt.figure(figsize=(10,10))
sns.regplot(expected_y, predicted_y, fit_reg=True, scatter_kws={"s": 100})

features = X_train.columns
importances = model_CBR.feature_importances_
indices = np.argsort(importances)

#Plot the figure
plt.figure(figsize=(10,20))
plt.title('Feature Importance')
plt.barh(range(len(indices)), importances[indices], color='blue', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')

plt.show()

Error_estimate(expected_yt, predicted_yt)
ztrain3=my_outputdic(Modell,MSE,RMSE,MAE,r2,Adjusted_R2)
ztrain3

training_df=training_df.append(ztrain3,ignore_index=True)
training_df

Error_estimate(expected_y, predicted_y)
ztest3=my_outputdic(Modell,MSE,RMSE,MAE,r2,Adjusted_R2)
ztest3

test_df=test_df.append(ztest3,ignore_index=True)
test_df

"""#**XG Boosting**"""

#importing the required packages and classes
from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split
from xgboost import XGBRegressor
import xgboost as xgb

Modell="XG Boosting"

xgb_model = XGBRegressor()
xgb_model.fit(X_train,y_train)

y_pred_train_x = xgb_model.predict(X_train)
y_pred_test_x= xgb_model.predict(X_test)

Error_estimate(y_train,y_pred_train_x )
ztrain4=my_outputdic(Modell,MSE,RMSE,MAE,r2,Adjusted_R2)
ztrain4

training_df=training_df.append(ztrain4,ignore_index=True)
training_df

Error_estimate(y_test, y_pred_test_x)
ztest4=my_outputdic(Modell,MSE,RMSE,MAE,r2,Adjusted_R2)
ztest4

test_df=test_df.append(ztest4,ignore_index=True)
test_df

### Heteroscadacity
plt.scatter((y_pred_test_x),(y_test)-(y_pred_test_x))

xgb_model.feature_importances_

features = X_train.columns
importances = xgb_model.feature_importances_
indices = np.argsort(importances)

#Plot the figure
plt.figure(figsize=(10,20))
plt.title('Feature Importance')
plt.barh(range(len(indices)), importances[indices], color='blue', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')

plt.show()



"""#**XG boosting(tunned)**#

### **Provide the range of values for chosen hyperparameters**
"""

parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower
              'objective':['reg:linear'],
              'learning_rate': [.03, 0.05, .07], #so called `eta` value
              'max_depth': [5, 6, 7],
              'min_child_weight': [4],
              'silent': [1],
              'subsample': [0.7],
              'colsample_bytree': [0.7],
              'n_estimators': [250,500,750,1000]}

from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split
from xgboost import XGBRegressor
import xgboost as xgb

from sklearn.model_selection import GridSearchCV
# Create an instance of the GradientBoostingRegressor
xgb_model = XGBRegressor()

# Grid search

xgb_grid = GridSearchCV(estimator=xgb_model,
                       param_grid = parameters,
                       cv =2,n_jobs=5,verbose=2)

xgb_grid.fit(X_train,y_train)

Modell="XG Boosting(tuned)"

print(xgb_grid.best_score_)
print(xgb_grid.best_params_)

xgb_grid.best_estimator_

xgb_optimal_model = xgb_grid.best_estimator_

xgb_grid.best_params_

# Making predictions on train and test data

y_pred_train_x_g = xgb_optimal_model.predict(X_train)
y_pred_test_x_g= xgb_optimal_model.predict(X_test)
print(y_pred_train_x_g)
print(y_pred_test_x_g)

Error_estimate(y_train,y_pred_train_x_g)
ztrain5=my_outputdic(Modell,MSE,RMSE,MAE,r2,Adjusted_R2)
ztrain5

training_df=training_df.append(ztrain5,ignore_index=True)
training_df

Error_estimate(y_test, y_pred_test_x_g)
ztest5=my_outputdic(Modell,MSE,RMSE,MAE,r2,Adjusted_R2)
ztest5

test_df=test_df.append(ztest5,ignore_index=True)
test_df

### Heteroscadacity
plt.scatter((y_pred_test_x_g),(y_test)-(y_pred_test_x_g))

xgb_optimal_model.feature_importances_

Importances = xgb_optimal_model.feature_importances_

Importance_dict = {'Feature' : list(X_train.columns),
                   'Feature Importance' : Importances}

Importance_df = pd.DataFrame(Importance_dict)

Importance_df['Feature Importance'] = round(Importance_df['Feature Importance'],2)

Importance_df.head()

Importance_df.sort_values(by=['Feature Importance'],ascending=False)

xgb_model.fit(X_train,y_train)

features = X_train.columns
Importances = xgb_model.feature_importances_
Indices = np.argsort(importances)

#Plot the figure
plt.figure(figsize=(10,15))
plt.title('Feature Importance')
plt.barh(range(len(Indices)), Importances[Indices], color='blue', align='center')
plt.yticks(range(len(Indices)), [features[i] for i in Indices])
plt.xlabel('Relative Importances')



"""# **Conclusion**

During the time of our analysis, we initially did EDA on all the features of our datset. We first analysed our dependent variable, 'Rented Bike Count' and also transformed it. Next we analysed categorical variable and dropped the variable who had majority of one class, we also analysed numerical variable, found out the correlation, distribution and their relationship with the dependent variable. We also removed some numerical features who had mostly 0 values and hot encoded the categorical variables.

Next we implemented 7 machine learning algorithms Linear Regression,lasso,ridge,elasticnet,decission tree, Random Forest and XGBoost. We did hyperparameter tuning to improve our model performance. The results of our evaluation are:
"""

# displaying the results of evaluation metric values for all models
result=pd.concat([training_df,test_df],keys=['Training set','Test set'])
result

"""• No overfitting is seen.

• Random forest Regressor and Gradient Boosting gridsearchcv gives the  R2 score of 99% and 95% recpectively for Train Set and 92% for Test set.and xgboosting 99% and 94% on highest

• Feature Importance value for Random Forest,XG boost and Gradient Boost are different.

• We can deploy this model.

However, this is not the ultimate end. As this data is time dependent, the values for variables like temperature, windspeed, solar radiation etc., will not always be consistent. Therefore, there will be scenarios where the model might not perform well. As Machine learning is an exponentially evolving field, we will have to be prepared for all contingencies and also keep checking our model from time to time. Therefore, having a quality knowledge and keeping pace with the ever evolving ML field would surely help one to stay a step ahead in future.

### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***
"""